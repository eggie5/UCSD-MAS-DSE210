{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handwritten digit recognition using a Gaussian generative model. In class, we mentioned the MNIST data set of handwritten digits. You can obtain it from: \n",
    "                          http://yann.lecun.com/exdb/mnist/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, you will build a classifier for this data, by modeling each class as a multivariate (784-dimensional) Gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each example in the mnist dataset is a 28x28 pixel image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "28*28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.sqrt(784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 10 classes to classify: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9. Each class is modeled by a multivariate 784-dimensional gaussian. In other words we have a 784-dimensional feature vector and 10 768 degree multivariate gaussians."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a smaller scale we can do this w/ the iris dataset. 4 Features/target. 4 classes. So each class would have a 4D M-Gaussian. This is actually a really good model for IRIS b/c gaussians follow the random patterns in nature and iris is samples from nature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the Iris data, we have 150 examples, with 3 total classes. Therefore the class priori or $Ï€_j = 1/3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n",
      "[ 50.  50.  50.]\n",
      "[ 0.33333333  0.33333333  0.33333333]\n",
      "[[ 0.121764  0.142276  0.029504  0.011264]\n",
      " [ 0.261104  0.0965    0.2164    0.038324]\n",
      " [ 0.396256  0.101924  0.298496  0.073924]]\n"
     ]
    }
   ],
   "source": [
    "print gnb.classes_\n",
    "print gnb.class_count_\n",
    "print gnb.class_prior_\n",
    "print gnb.sigma_ # variance of each feature per class -- not Sigma in the context of covariance matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what scikit did was to fit a gaussian to each of the classes (3) in the training set examples. It does that by finding the mean and the covariance from the examples. Let the Gaussian for the jth class be: $P_j = N(\\mu_j, \\Sigma_j)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\DeclareMathOperator*{\\argmax}{arg\\,max}$\n",
    "Then in order to classify an unknown flower, simply use the class prior w/ the class posterior (bayes rule) fo all the classes and choose the one w/ the largest probability:\n",
    "\n",
    "$$\\argmax \\pi_j*P_j$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 150 points : 6\n"
     ]
    }
   ],
   "source": [
    "y_pred = gnb.predict(iris.data)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (iris.data.shape[0],(iris.target != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy huh? Now, the devil is in the details: how do you calculate $P_j$, how do we estimate a gaussian for each class j?\n",
    "\n",
    "$$P_j = N(\\mu_j, \\Sigma_j)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that $N(\\mu_j, \\Sigma_j)$, the probability density function (PDF) of a multivariate Gaussian is this standard form:\n",
    "\n",
    "$$\n",
    "P(\\hat{X}) = \\frac{1}{\\sqrt{(2 \\pi)^k \\det \\Sigma}} \\exp\\left( -\\frac{1}{2} (x - \\mu)^T \\Sigma^{-1} (x - \\mu) \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "Therefore the trick is to tune $\\Sigma$, the covariance matrix, to model the trainitg data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Sigma$ is a p x p matrix containing all pairwise covariances, where p is the number of features in your training set:\n",
    "\n",
    "$$\n",
    "\\Sigma_{ij} = \\Sigma_{ji} = cov(X_i, X_j)    if i!=j\n",
    "$$\n",
    "$$\n",
    "\\Sigma_{ii} = var(X_i)\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    var(x_1) & cov(x_1,x_2) & \\dots  & x_{1n} \\\\\n",
    "    cov(x_2) & var(x_2) & \\dots  & x_{2n} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    x_{d1} & x_{d2} & \\dots  & var(x_n)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then, given some training points, the way to generate this matrix is marked w/ a start on my notes:\n",
    "\n",
    "for each target/class (10) you will get a $\\mu$ and a $\\Sigma$ that you can pop into the numpy PDF routine: `np.random.multivariate_normal`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#numpy np.random.multivariate_normal routine exmaple\n",
    "mean = [0, 0]\n",
    "cov = [[9, 0], [0, 1]]  \n",
    "x,y = np.random.multivariate_normal(mean, cov, 100).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "For mnist dataset we will have 4x4 covariance matrix b/c there are 4 features in our training set., here's an example of finding the covariance matrix $\\Sigma$ on the iris trainging set. We should be abel to confirm our answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets split into a test and training set\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(iris.data,iris.target, test_size=0.4, random_state=4)\n",
    "\n",
    "# lets gather all the examples from class 0\n",
    "def get_examples_for_class(class_id):\n",
    "    examples = []\n",
    "    for i, example in enumerate(X_train):\n",
    "        if Y_train[i]==class_id:\n",
    "            examples.append(example)\n",
    "        \n",
    "    examples = np.matrix(examples)\n",
    "    return examples\n",
    "\n",
    "examples = get_examples_for_class(0)\n",
    "examples.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now according to the above assertion this should map to a 4x4 covariance matrix. We can use `numpy.cov` to test this assertion and then look at how to implement the equivilent `numpy.cov` in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.964  3.416  1.44   0.24 ]\n",
      "[[ 0.1049      0.06976667  0.01483333  0.00566667]\n",
      " [ 0.06976667  0.10723333  0.00391667  0.00266667]\n",
      " [ 0.01483333  0.00391667  0.02583333  0.00625   ]\n",
      " [ 0.00566667  0.00266667  0.00625     0.01      ]]\n"
     ]
    }
   ],
   "source": [
    "mean = np.array(examples.mean(0))[0]\n",
    "cov = np.cov(examples.T) # I don't know why you have to transpose the input to numpy... \n",
    "print mean\n",
    "print cov #should be 4x4 for iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we can take this covariance matrix and pipe it into the numpy PDF routine to get our distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "P_0 = np.random.multivariate_normal(mean, cov).T\n",
    "P_0\n",
    "\n",
    "var = multivariate_normal(mean=mean, cov=cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now according to equation 1, if we want to classify some vector, X,\n",
    "\n",
    "$$\\argmax_j \\pi_j*P_j(X)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets test the probabity the the following test vector is in class 0, or:\n",
    "\n",
    "$$ \\pi_0*P_0(X) $$\n",
    "\n",
    "where, X is the vector below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.7  4.4  1.5  0.4]\n"
     ]
    }
   ],
   "source": [
    "X1=X_test[15]\n",
    "import random\n",
    "X1 = random.choice (X_test)\n",
    "print X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0.08678207121797761]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior = pi_0 = gnb.class_prior_[0]\n",
    "prob_0=[0, var.pdf(X1)]\n",
    "prob_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P_1 = N(\\mu_1, \\Sigma_1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3.0864750077334886e-44]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now for class 1\n",
    "examples_1 = get_examples_for_class(1)\n",
    "mean_1 = np.array(examples_1.mean(0))[0]\n",
    "cov_1 = np.cov(examples_1.T)\n",
    "p_x_1 = multivariate_normal(mean=mean_1, cov=cov_1)\n",
    "prob_1 = [1, gnb.class_prior_[1] * p_x_1.pdf(X1)]\n",
    "prob_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3.4459557583437947e-66]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now for class 2\n",
    "examples_2 = get_examples_for_class(2)\n",
    "mean_2 = np.array(examples_2.mean(0))[0]\n",
    "cov_2 = np.cov(examples_2.T)\n",
    "p_x_2 = multivariate_normal(mean=mean_2, cov=cov_2)\n",
    "prob_2 = [2, gnb.class_prior_[2] * p_x_2.pdf(X1)]\n",
    "prob_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setosa\n"
     ]
    }
   ],
   "source": [
    "prediction = max(prob_0, prob_1, prob_2, key= lambda a: a[1])\n",
    "print iris.target_names[prediction[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setosa\n"
     ]
    }
   ],
   "source": [
    "X = iris.data\n",
    "Y = iris.target\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "clf.fit(X, Y)\n",
    "\n",
    "print(iris.target_names[clf.predict([X1])][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it seems to be a functioning classifier. Now lets scale this up and test it on the digits dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Digits should have 10 classes: 0-9 with each example having 784 (representing 28x28 pixel image). A note: scikit's version of nmist is only 8x8 whereas the original is 28*28. SO as you can see above we have 64 features instead of 784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1078, 64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits = datasets.load_digits()\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(digits.data, digits.target, test_size=0.4, random_state=4)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to calculate the prior probabilites of the 10 classes. For the sake of lazyness well just use NBClassifer to gen these values for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.09369202,  0.10482375,  0.0974026 ,  0.0974026 ,  0.10575139,\n",
       "        0.10296846,  0.0974026 ,  0.10018553,  0.09925788,  0.10111317])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "helper = GaussianNB()\n",
    "helper.fit(X_train, Y_train)\n",
    "classes = helper.classes_\n",
    "priors = helper.class_prior_\n",
    "priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So from the above we can see the baysian prior probabilities for the 10 classes 0-9 whick looks like a pretty even distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Now lets generate the baysian posteror probabilities for each class using the training data.\n",
    "\n",
    "See this note on the covariance matrix tuning: http://stackoverflow.com/questions/35273908/scipy-stats-multivariate-normal-raising-linalgerror-singular-matrix-even-thou/35293215"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "posteriors=[]\n",
    "\n",
    "for klass in classes:\n",
    "    examples = get_examples_for_class(klass)\n",
    "    mean = np.array(examples.mean(0))[0]\n",
    "    cov = np.cov(examples.T)\n",
    "    p_x = multivariate_normal(mean=mean, cov=cov, allow_singular=True)\n",
    "    posteriors.append(p_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "Now that we have the prior and posterior probabilities for our training set, lets use that to make a prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.   0.   0.   0.   6.  15.   2.   0.   0.   0.   0.   5.  16.  16.   2.\n",
      "   0.   0.   0.   4.  16.  12.  16.   0.   0.   0.   4.  15.   6.   7.  13.\n",
      "   0.   0.   0.  11.  15.  15.  16.  16.   9.   0.   0.   9.  13.  12.  13.\n",
      "  14.   3.   0.   0.   0.   0.   0.   9.   8.   0.   0.   0.   0.   0.   0.\n",
      "   8.   8.   0.   0.]\n"
     ]
    }
   ],
   "source": [
    "#choose a random point from the test data\n",
    "x = random.choice (X_test)\n",
    "print x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0.0],\n",
       " [1, 5.8802340424920592e-222],\n",
       " [2, 0.0],\n",
       " [3, 0.0],\n",
       " [4, 9.9863047311482061e-62],\n",
       " [5, 0.0],\n",
       " [6, 0.0],\n",
       " [7, 0.0],\n",
       " [8, 0.0],\n",
       " [9, 0.0]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayes_probs = []\n",
    "for klass in classes:\n",
    "    prob = [klass, priors[klass] * posteriors[klass].pdf(x)]\n",
    "    bayes_probs.append(prob)\n",
    "bayes_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the probabilities are VERY small. In this discrete environment we may be losing a lot of percission, so it may be wise to explore using the `log_pdf` instead. We will explore this more later.\n",
    "\n",
    "Now we choose the max and that is our prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "prediction = max(bayes_probs, key= lambda a: a[1])\n",
    "print digits.target_names[prediction[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our routine predicted a 4, lets visually confirm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMAAAADDCAYAAADUSB6pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11289a1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1, figsize=(3, 3))\n",
    "plt.imshow(x.reshape(8,8), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark\n",
    "\n",
    "Now lets scale this up and check our error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#first I need an interface to batch test, test input (instead of 1 vector at a time, like above)\n",
    "\n",
    "Y = []\n",
    "for x in X_test:\n",
    "    bayes_probs = []\n",
    "    for klass in classes:\n",
    "        prob = [klass, priors[klass] * posteriors[klass].pdf(x)]\n",
    "        bayes_probs.append(prob)\n",
    "    prediction = max(bayes_probs, key= lambda a: a[1])\n",
    "    Y.append(prediction[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate: 34/719 = 0.047288\n"
     ]
    }
   ],
   "source": [
    "errors = (Y_test != Y).sum()\n",
    "total = X_test.shape[0]\n",
    "print(\"Error rate: %d/%d = %f\" % ((errors,total,(errors/float(total)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1078, 64)\n",
      "(719, 64)\n",
      "(1797, 64)\n"
     ]
    }
   ],
   "source": [
    "print X_train.shape\n",
    "print X_test.shape\n",
    "print digits.data.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
